{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9cff04-f8f1-486b-b944-886462c94ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import tree\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import compose\n",
    "from river import naive_bayes\n",
    "from time import time\n",
    "\n",
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "import pickle\n",
    "import ray\n",
    "import pandas as pd\n",
    "from pathlib import Path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0c2ef-7361-46fa-bb9c-827021d9da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f(topic,max_cnt_per_consumer,no_of_consumers,consumer_no,total_no_of_partitions,bts,u,p):\n",
    "    \n",
    "    max_cnt=max_cnt_per_consumer\n",
    "    import json\n",
    "    import sys\n",
    "    import os\n",
    "    from json import dumps, loads    \n",
    "    from confluent_kafka.cimpl import Producer, Consumer, KafkaException, KafkaError\n",
    "\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from confluent_kafka import TopicPartition\n",
    "    from river import datasets\n",
    "    from river import metrics\n",
    "    from river import tree\n",
    "    from river import ensemble\n",
    "    from river import evaluate\n",
    "    from river import compose\n",
    "    from river import naive_bayes\n",
    "    from time import time\n",
    "\n",
    "    from river import anomaly\n",
    "    from river import compose\n",
    "    from river import datasets\n",
    "    from river import metrics\n",
    "    from river import preprocessing\n",
    "    import pickle\n",
    "    import statistics\n",
    "    import certifi\n",
    "    tls = []\n",
    "    partitions_per_consumer = int(total_no_of_partitions/no_of_consumers)\n",
    "    for t in range(partitions_per_consumer):        \n",
    "        tls.insert(t, TopicPartition(topic, consumer_no*partitions_per_consumer+t))\n",
    "    group_id=f\"group-{no_of_consumers}\"\n",
    "    \n",
    "    conf = {'bootstrap.servers': bts,\n",
    "                'sasl.mechanism': 'PLAIN',\n",
    "                'security.protocol': 'SASL_SSL',\n",
    "                'sasl.username': u,\n",
    "                'sasl.password': p,\n",
    "                'ssl.ca.location': certifi.where(),\n",
    "                'group.id': group_id,\n",
    "                'auto.offset.reset': 'latest'}\n",
    "    consumer = Consumer(conf)\n",
    "    consumer.assign(tls)\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    durs=[]\n",
    "    i=0\n",
    "    ignored=0\n",
    "    mem=[]\n",
    "    model = None\n",
    "    try:\n",
    "        if topic=='HoeffdingAdaptiveTreeClassifier':\n",
    "            model = tree.HoeffdingAdaptiveTreeClassifier(grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10,seed=0)\n",
    "        elif topic=='SRPClassifierHAT':\n",
    "            model =  ensemble.SRPClassifier(\n",
    "                                               model=tree.HoeffdingAdaptiveTreeClassifier(grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10, seed=0), seed=42,\n",
    "                                            )\n",
    "        elif topic=='SRPClassifierNaiveBayes':\n",
    "            model = ensemble.SRPClassifier(\n",
    "                                             model=naive_bayes.BernoulliNB(alpha=1), seed=42,\n",
    "                                          )\n",
    "        elif topic=='AdaptiveRandomForestClassifier':\n",
    "            model = ensemble.AdaptiveRandomForestClassifier(leaf_prediction=\"mc\")\n",
    "                                          \n",
    "        elif topic=='HalfSpaceTrees':\n",
    "            model = compose.Pipeline(preprocessing.MinMaxScaler(),anomaly.HalfSpaceTrees(seed=42))\n",
    "            \n",
    "        while(i<max_cnt):\n",
    "            msg = consumer.poll(timeout=0.1)\n",
    "            if msg is None: continue\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    # End of partition event\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                         (msg.topic(), msg.partition(), msg.offset()))\n",
    "            else:                \n",
    "                message = loads(msg.value().decode(\"utf-8\"))\n",
    "                st = message['st']\n",
    "                data = message['f']\n",
    "                y = (message['y']=='true')              \n",
    "               \n",
    "                try:                    \n",
    "                    score = model.predict_one(data)\n",
    "                    model = model.learn_one(data,y)                    \n",
    "                    end = time()\n",
    "                    auc = auc.update(y, score)\n",
    "                    f1 = f1.update(y, score)\n",
    "                    recall = recall.update(y, score)\n",
    "                    durs.append(end-st)\n",
    "                    i = i + 1                \n",
    "                    if i%1000==0: #Sample model memory every 1000 records\n",
    "                        mem.append(model._raw_memory_usage)\n",
    "                except:\n",
    "                    ignored = ignored + 1\n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        consumer.close()    \n",
    "    mean = statistics.mean(durs)\n",
    "    median = statistics.median(durs)\n",
    "    max1 = max(durs)\n",
    "    min2 = min(durs)\n",
    "    total_records = len(durs)\n",
    "    memory_usage = statistics.mean(mem)\n",
    "    return {'index':consumer_no,'ignored':ignored, 'max':max1,'min':min2,'avg':mean,'median':median,'length':total_records,'auc':auc,'f1':f1,'recall':recall,'avg_model_mem':memory_usage}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea81b91-c49c-4720-b8bc-3bf4d6d25713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6786d14-7ff8-4b15-873f-96b302568de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ray.is_initialized() )\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "if ray.is_initialized() == False:\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    ray.init(f\"ray://{service_host}:{service_port}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694af36-2a14-43fc-bb83-cd59ac949ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace topic and no_of_consumers as needed\n",
    "max_records = 100000\n",
    "topic='HoeffdingAdaptiveTreeClassifier'\n",
    "no_of_partitions = 16\n",
    "no_of_consumers = 8\n",
    "max_cnt_per_consumer=int(max_records/no_of_consumers)\n",
    "\n",
    "\n",
    "b = os.environ['kafka_bootstrap_servers']\n",
    "u = os.environ['kafka_username']\n",
    "p = os.environ['kafka_password']\n",
    "futures = [f.remote(topic,max_cnt_per_consumer,no_of_consumers,consumer_no,no_of_partitions,b,u,p) for consumer_no in range(no_of_consumers)]\n",
    "results = ray.get(futures)\n",
    "print(results)\n",
    "#Start the producer from the file confluence_producer_malicious_url.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e2322-e29d-4826-8b55-e6cee68517df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame.from_records(results)\n",
    "filepath = Path(f'/mnt/code/results/metrics-{topic}-{no_of_consumers}.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)  \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e24c3-6c62-47b4-9bf8-d34fc1114d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f_cc(topic,max_cnt_per_consumer,no_of_consumers,consumer_no,total_no_of_partitions,bts,u,p):\n",
    "    \n",
    "    max_cnt=max_cnt_per_consumer\n",
    "    import json\n",
    "    import sys\n",
    "    import os\n",
    "    from json import dumps, loads    \n",
    "    from confluent_kafka.cimpl import Producer, Consumer, KafkaException, KafkaError\n",
    "\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from confluent_kafka import TopicPartition\n",
    "    from river import datasets\n",
    "    from river import metrics\n",
    "    from river import tree\n",
    "    from river import ensemble\n",
    "    from river import evaluate\n",
    "    from river import compose\n",
    "    from river import naive_bayes\n",
    "    from time import time\n",
    "\n",
    "    from river import anomaly\n",
    "    from river import compose\n",
    "    from river import datasets\n",
    "    from river import metrics\n",
    "    from river import preprocessing\n",
    "    import pickle\n",
    "    import statistics\n",
    "    import certifi\n",
    "    tls = []\n",
    "    partitions_per_consumer = int(total_no_of_partitions/no_of_consumers)\n",
    "    for t in range(partitions_per_consumer):        \n",
    "        tls.insert(t, TopicPartition(topic, consumer_no*partitions_per_consumer+t))\n",
    "    group_id=f\"group-{no_of_consumers}\"\n",
    "    \n",
    "    conf = {'bootstrap.servers': bts,\n",
    "                'sasl.mechanism': 'PLAIN',\n",
    "                'security.protocol': 'SASL_SSL',\n",
    "                'sasl.username': u,\n",
    "                'sasl.password': p,\n",
    "                'ssl.ca.location': certifi.where(),\n",
    "                'group.id': group_id,\n",
    "                'auto.offset.reset': 'latest'}\n",
    "    consumer = Consumer(conf)\n",
    "    consumer.assign(tls)\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    durs=[]\n",
    "    i=0\n",
    "    ignored=0\n",
    "    mem=[]\n",
    "    model = None\n",
    "    try:\n",
    "        if topic=='HalfSpaceTrees':\n",
    "            model = compose.Pipeline(preprocessing.MinMaxScaler(),anomaly.HalfSpaceTrees(seed=42))\n",
    "            \n",
    "        while(i<max_cnt):\n",
    "            \n",
    "            msg = consumer.poll(timeout=0.1)\n",
    "            if msg is None: continue\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    # End of partition event\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                         (msg.topic(), msg.partition(), msg.offset()))\n",
    "            else:                \n",
    "                message = loads(msg.value().decode(\"utf-8\"))\n",
    "                st = message['st']\n",
    "                data = message['f']\n",
    "    \n",
    "                y = (message['y']==1)        \n",
    "                try:                    \n",
    "                    score = model.score_one(data) \n",
    "                    model = model.learn_one(data)\n",
    "\n",
    "                    end = time()   \n",
    "                    auc = auc.update(y, score==1)\n",
    "                    f1 = f1.update(y, score==1)\n",
    "                    recall = recall.update(y, score==1)\n",
    "                    durs.append(end-st)\n",
    "                    i = i + 1                \n",
    "                    if i%1000==0: #Sample model memory every 1000 records\n",
    "                        mem.append(model._raw_memory_usage)\n",
    "                except:\n",
    "                    ignored = ignored + 1\n",
    "    except:\n",
    "        print('exception')\n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        consumer.close()    \n",
    "    mean = statistics.mean(durs)\n",
    "    median = statistics.median(durs)\n",
    "    max1 = max(durs)\n",
    "    min2 = min(durs)\n",
    "    total_records = len(durs)\n",
    "    memory_usage = statistics.mean(mem)\n",
    "    return {'index':consumer_no,'ignored':ignored, 'max':max1,'min':min2,'avg':mean,'median':median,'length':total_records,'auc':auc,'f1':f1,'recall':recall,'avg_model_mem':memory_usage}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "596b2c60-22fa-4379-a8ff-64d3f7746322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(ray.is_initialized() )\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "if ray.is_initialized() == False:\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    ray.init(f\"ray://{service_host}:{service_port}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb280154-ae80-4a3f-b265-e3438635c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 0, 'ignored': 0, 'max': 0.7428181171417236, 'min': 0.047324419021606445, 'avg': 0.08125932992935181, 'median': 0.08186662197113037, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.77%, 'avg_model_mem': 1003542.3333333334}, {'index': 1, 'ignored': 0, 'max': 0.7220723628997803, 'min': 0.047197580337524414, 'avg': 0.08225127578735351, 'median': 0.08317673206329346, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.78%, 'avg_model_mem': 1003477}, {'index': 2, 'ignored': 0, 'max': 0.7327229976654053, 'min': 0.04799628257751465, 'avg': 0.08202138759613037, 'median': 0.08086156845092773, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.77%, 'avg_model_mem': 1003684.6666666666}, {'index': 3, 'ignored': 0, 'max': 0.7649078369140625, 'min': 0.047894954681396484, 'avg': 0.08434619976043702, 'median': 0.08265852928161621, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.78%, 'avg_model_mem': 1003610}, {'index': 4, 'ignored': 0, 'max': 0.18736720085144043, 'min': 0.04974484443664551, 'avg': 0.08219844192504883, 'median': 0.08538234233856201, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.81%, 'avg_model_mem': 1003687}, {'index': 5, 'ignored': 0, 'max': 0.17241716384887695, 'min': 0.045824289321899414, 'avg': 0.07759632074356079, 'median': 0.08151125907897949, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.80%, 'avg_model_mem': 1003661.3333333334}, {'index': 6, 'ignored': 0, 'max': 0.35911989212036133, 'min': 0.04754018783569336, 'avg': 0.08326140872955322, 'median': 0.08345901966094971, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.78%, 'avg_model_mem': 1003129.3333333334}, {'index': 7, 'ignored': 0, 'max': 0.1703357696533203, 'min': 0.04647374153137207, 'avg': 0.07588535879135132, 'median': 0.08000683784484863, 'length': 12500, 'auc': ROCAUC: 50.00%, 'f1': F1: 0.00%, 'recall': MicroRecall: 99.71%, 'avg_model_mem': 1003666}]\n"
     ]
    }
   ],
   "source": [
    "#Replace topic and no_of_consumers as needed\n",
    "max_records = 100000\n",
    "topic='HalfSpaceTrees'\n",
    "no_of_partitions = 16\n",
    "no_of_consumers = 8\n",
    "max_cnt_per_consumer=int(max_records/no_of_consumers)\n",
    "\n",
    "\n",
    "b = os.environ['kafka_bootstrap_servers']\n",
    "u = os.environ['kafka_username']\n",
    "p = os.environ['kafka_password']\n",
    "futures = [f_cc.remote(topic,max_cnt_per_consumer,no_of_consumers,consumer_no,no_of_partitions,b,u,p) for consumer_no in range(no_of_consumers)]\n",
    "results = ray.get(futures)\n",
    "print(results)\n",
    "#Start the producer from the file confluence_producer_credit)card.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31e6051f-2f53-4b4f-8b7d-c87bd812e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame.from_records(results)\n",
    "filepath = Path(f'/mnt/code/results/metrics-{topic}-{no_of_consumers}.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)  \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aa06959-a1de-4281-be4d-a22dee1de167",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3138289115.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [36]\u001b[0;36m\u001b[0m\n\u001b[0;31m    futures = [f.remote(topic,max_cnt_per_consumer,no_of_consumers,consumer_no,b,u,p) for consumer_no in range(no_of_consumers)]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#If fully automated run this after filling out he missing lines. This cell is intended to give\n",
    "#an idea of all the experiments that are run for the malicious url use-case\n",
    "max_records=100000\n",
    "b = os.environ['kafka_bootstrap_servers']\n",
    "u = os.environ['kafka_username']\n",
    "p = os.environ['kafka_password']\n",
    "for topic in ['HoeffdingAdaptiveTreeClassifier','SRPClassifierHAT','SRPClassifierNaiveBayes','AdaptiveRandomForestClassifier','HalfSpaceTrees']:\n",
    "    for no_of_consumers in [8,4,2]:\n",
    "        #Create Topic here\n",
    "        #Start producing to the topic here in a separate thread\n",
    "        \n",
    "        if ray.is_initialized():\n",
    "            ray.shutdown()\n",
    "        if ray.is_initialized() == False:\n",
    "            service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "            service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "            ray.init(f\"ray://{service_host}:{service_port}\")\n",
    "        max_cnt_per_consumer=100000/no_of_consumers\n",
    "        print('Processing Topic'\n",
    "        futures = [f.remote(topic,max_cnt_per_consumer,no_of_consumers,consumer_no,b,u,p) for consumer_no in range(no_of_consumers)]\n",
    "        results = ray.get(futures)\n",
    "        df= pd.DataFrame.from_records(results)\n",
    "        filepath = Path(f'/mnt/code/results/metrics-{topic}-{no_of_consumers}.csv')  \n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "        df.to_csv(filepath)  \n",
    "        #Delete topic\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53efb6-74a9-447a-b137-4b7047a8ac12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
