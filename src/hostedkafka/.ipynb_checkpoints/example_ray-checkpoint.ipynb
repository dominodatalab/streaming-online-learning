{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf1de0f-f961-4519-918e-9eb70f043e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_root_path = '/mnt/data/streaming-online-learning'\n",
    "model_path = os.path.join(dataset_root_path,'models/extremelyfastdecisiontreeclassifier/v0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "420c61a5-682e-4c05-a27d-bbf3c47d439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import tree\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import compose\n",
    "from river import naive_bayes\n",
    "from time import time\n",
    "\n",
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d32bdf0-0cdf-46ef-8755-286de1cfebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"HoeffdingAdaptiveTreeClassifier\": tree.HoeffdingAdaptiveTreeClassifier(\n",
    "                                            grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10,\n",
    "                                            seed=0),\n",
    "    \"SRPClassifier - HAT\":              ensemble.SRPClassifier(\n",
    "                                           model=tree.HoeffdingAdaptiveTreeClassifier(grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10, seed=0), seed=42,\n",
    "                                        ),\n",
    "    \"SRPClassifier - Naive Bayes\":    ensemble.SRPClassifier(\n",
    "                                         model=naive_bayes.BernoulliNB(alpha=1), seed=42,\n",
    "                                      ),\n",
    "    \"AdaptiveRandomForestClassifier\": ensemble.AdaptiveRandomForestClassifier(leaf_prediction=\"mc\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c70f5e4a-5a5d-483f-ac3a-204b4754a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoeffdingAdaptiveTreeClassifier\n",
      "Now processing element 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::f()\u001b[39m (pid=205, ip=10.0.57.151)\n",
      "  File \"/tmp/ipykernel_789/2046560509.py\", line 78, in f\n",
      "NameError: name 'end2' is not defined\n",
      "Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::f()\u001b[39m (pid=205, ip=10.0.50.21)\n",
      "  File \"/tmp/ipykernel_789/2046560509.py\", line 78, in f\n",
      "NameError: name 'end2' is not defined\n",
      "Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::f()\u001b[39m (pid=205, ip=10.0.53.101)\n",
      "  File \"/tmp/ipykernel_789/2046560509.py\", line 78, in f\n",
      "NameError: name 'end2' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing element 1000\n",
      "Now processing element 2000\n",
      "Now processing element 3000\n",
      "Now processing element 4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m st\u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m message \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     23\u001b[0m message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mx\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/hoeffding_adaptive_tree_classifier.py:216\u001b[0m, in \u001b[0;36mHoeffdingAdaptiveTreeClassifier.learn_one\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_leaf()\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_active_leaves \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_weight_seen_by_model \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_estimate_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_model_size()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/nodes/hatc_nodes.py:249\u001b[0m, in \u001b[0;36mAdaBranchClassifier.learn_one\u001b[0;34m(self, x, y, sample_weight, tree, parent, parent_branch)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Learn one sample in alternate tree and child nodes\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alternate_tree:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_alternate_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_branch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_branch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/nodes/hatc_nodes.py:73\u001b[0m, in \u001b[0;36mAdaLeafClassifier.learn_one\u001b[0;34m(self, x, y, sample_weight, tree, parent, parent_branch)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mean_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mean_error\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Update statistics\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m weight_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_weight\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_seen \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_split_attempt_at \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mgrace_period:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/nodes/htc_nodes.py:181\u001b[0m, in \u001b[0;36mLeafNaiveBayesAdaptive.learn_one\u001b[0;34m(self, x, y, sample_weight, tree)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nb_pred) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(nb_pred, key\u001b[38;5;241m=\u001b[39mnb_pred\u001b[38;5;241m.\u001b[39mget) \u001b[38;5;241m==\u001b[39m y:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nb_correct_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/nodes/leaf.py:174\u001b[0m, in \u001b[0;36mHTLeaf.learn_one\u001b[0;34m(self, x, y, sample_weight, tree)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_stats(y, sample_weight)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_active():\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_splitters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnominal_attributes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/nodes/leaf.py:109\u001b[0m, in \u001b[0;36mHTLeaf.update_splitters\u001b[0;34m(self, x, y, sample_weight, nominal_attributes)\u001b[0m\n\u001b[1;32m    106\u001b[0m         splitter \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitters[att_id] \u001b[38;5;241m=\u001b[39m splitter\n\u001b[0;32m--> 109\u001b[0m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/tree/splitter/gaussian_splitter.py:48\u001b[0m, in \u001b[0;36mGaussianSplitter.update\u001b[0;34m(self, att_val, target_val, sample_weight)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_per_class[target_val] \u001b[38;5;241m=\u001b[39m att_val\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_per_class[target_val] \u001b[38;5;241m=\u001b[39m att_val\n\u001b[0;32m---> 48\u001b[0m \u001b[43mval_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/river/proba/gaussian.py:61\u001b[0m, in \u001b[0;36mGaussian.update\u001b[0;34m(self, x, w)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var\u001b[49m\u001b[38;5;241m.\u001b[39mupdate(x, w)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = datasets.MaliciousURL()\n",
    "from time import time\n",
    "size=50000\n",
    "data = dataset.take(size)\n",
    "model = models['HoeffdingAdaptiveTreeClassifier']\n",
    "\n",
    "print(model)\n",
    "auc = metrics.ROCAUC()\n",
    "f1 = metrics.F1()\n",
    "recall = metrics.MicroRecall()\n",
    "start = time()\n",
    "end = 0\n",
    "i=0\n",
    "durs = []\n",
    "st = 0\n",
    "for x,y in data:\n",
    "    if(i%1000==0):\n",
    "        print(f\"Now processing element {i}\")\n",
    "    i = i + 1\n",
    "    st= time()\n",
    "    model = model.learn_one(x,y)\n",
    "    message = {}\n",
    "    message['f']=x\n",
    "    message['y']=y\n",
    "    score = model.predict_one(x)\n",
    "    \n",
    "    end = time()\n",
    "    if score:\n",
    "        auc = auc.update(y, score)\n",
    "        f1 = f1.update(y, score)\n",
    "        recall = recall.update(y, score)\n",
    "    durs.append(end-st)\n",
    "end = time()\n",
    "duration = end - start\n",
    "print('AUC, F1, Recall, TIME')\n",
    "auc, f1, recall, duration\n",
    "\n",
    "#pickle.dump( model, open( os.path.join(model_path,\"model.pkl\"), \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273dc018-9aff-4e35-bc89-d30f32c8aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "mean = statistics.mean(durs)\n",
    "median = statistics.median(durs)\n",
    "max1 = max(durs)\n",
    "min2 = min(durs)\n",
    "\n",
    "print('max=' + str(max1))\n",
    "print('min=' + str(min2))\n",
    "print('avg=' + str(mean))\n",
    "print('med=' + str(median))\n",
    "\n",
    "#print('total obs =' + str(len(durs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa218821-e990-4343-90b8-9edc5eea28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c350ed3b-aa0d-44cc-a166-56561b271d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee04b78-f894-42a0-8a8e-ab954c44143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    models = {\n",
    "        \"HoeffdingAdaptiveTreeClassifier\": tree.HoeffdingAdaptiveTreeClassifier(\n",
    "                                                grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10,\n",
    "                                                seed=0),\n",
    "        \"SRPClassifier - HAT\":              ensemble.SRPClassifier(\n",
    "                                               model=tree.HoeffdingAdaptiveTreeClassifier(grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10, seed=0), seed=42,\n",
    "                                            ),\n",
    "        \"SRPClassifier - Naive Bayes\":    ensemble.SRPClassifier(\n",
    "                                             model=naive_bayes.BernoulliNB(alpha=1), seed=42,\n",
    "                                          ),\n",
    "        \"AdaptiveRandomForestClassifier\": ensemble.AdaptiveRandomForestClassifier(leaf_prediction=\"mc\")\n",
    "    }\n",
    "    model = models['HoeffdingAdaptiveTreeClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51da9592-97a5-41c1-af04-15cc0edec10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import os\n",
    "print(ray.is_initialized() )\n",
    "if ray.is_initialized() == False:\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    ray.init(f\"ray://{service_host}:{service_port}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e0c2ef-7361-46fa-bb9c-827021d9da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f(x,bts,u,p):\n",
    "    import json\n",
    "    import sys\n",
    "    import os\n",
    "    from json import dumps, loads\n",
    "    \n",
    "    from confluent_kafka.cimpl import Producer, Consumer, KafkaException, KafkaError\n",
    "\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from confluent_kafka import TopicPartition\n",
    "    from river import datasets\n",
    "    from river import metrics\n",
    "    from river import tree\n",
    "    from river import ensemble\n",
    "    from river import evaluate\n",
    "    from river import compose\n",
    "    from river import naive_bayes\n",
    "    from time import time\n",
    "\n",
    "    from river import anomaly\n",
    "    from river import compose\n",
    "    from river import datasets\n",
    "    from river import metrics\n",
    "    from river import preprocessing\n",
    "    import pickle\n",
    "    import statistics\n",
    "    import certifi\n",
    "    tls = []\n",
    "    topic = 'f2'\n",
    "    tls.insert(0, TopicPartition(topic, x*2))\n",
    "    tls.insert(1, TopicPartition(topic, x*2+1))\n",
    "    group_id='c-group-2'\n",
    "    conf = {'bootstrap.servers': bts,\n",
    "                'sasl.mechanism': 'PLAIN',\n",
    "                'security.protocol': 'SASL_SSL',\n",
    "                'sasl.username': u,\n",
    "                'sasl.password': p,\n",
    "                'ssl.ca.location': certifi.where(),\n",
    "                'group.id': group_id,\n",
    "                'auto.offset.reset': 'latest'}\n",
    "    consumer = Consumer(conf)\n",
    "    consumer.assign(tls)\n",
    "    auc = metrics.ROCAUC()\n",
    "    f1 = metrics.F1()\n",
    "    recall = metrics.MicroRecall()\n",
    "    durs=[]\n",
    "    i=0\n",
    "    try:\n",
    "        model = tree.HoeffdingAdaptiveTreeClassifier(\n",
    "                                                grace_period=100,  delta=1e-5, leaf_prediction='nb', nb_threshold=10,\n",
    "                                                seed=0)\n",
    "        \n",
    "        while(i<10000):\n",
    "            msg = consumer.poll(timeout=0.1)\n",
    "            if msg is None: continue\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    # End of partition event\n",
    "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                         (msg.topic(), msg.partition(), msg.offset()))\n",
    "                elif msg.error():\n",
    "                    raise KafkaException(msg.error())\n",
    "            else:\n",
    "                i = i + 1\n",
    "                \n",
    "                message = loads(msg.value().decode(\"utf-8\"))\n",
    "                st = message['st']\n",
    "                data = message['f']\n",
    "                y = message['y']\n",
    "                #print(data)\n",
    "                #print(y)\n",
    "                model = model.learn_one(data,y)\n",
    "                score = model.predict_one(data)\n",
    "                end = time()\n",
    "                if score:\n",
    "                    auc = auc.update(y, score)\n",
    "                    f1 = f1.update(y, score)\n",
    "                    recall = recall.update(y, score)\n",
    "                durs.append(end-st)\n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        consumer.close()    \n",
    "    mean = statistics.mean(durs)\n",
    "    median = statistics.median(durs)\n",
    "    max1 = max(durs)\n",
    "    min2 = min(durs)\n",
    "    total_records = len(durs)\n",
    "    #print('max=' + str(max1))\n",
    "    #print('min=' + str(min2))\n",
    "    #print('avg=' + str(mean))\n",
    "    #print('med=' + str(median))\n",
    "    #print('AUC, F1, Recall, TIME')\n",
    "    #auc, f1, recall, duration\n",
    "    return {'index':x,'max':max1,'min':min2,'avg':mean,'median':median,'length':total_records,'auc':auc,'f1':f1,'recall':recall}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d694af36-2a14-43fc-bb83-cd59ac949ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 0, 'max': 0.2167196273803711, 'min': 0.0007634162902832031, 'avg': 0.007828619980812072, 'median': 0.005753278732299805, 'length': 10000, 'auc': ROCAUC: 50.00%, 'f1': F1: 93.14%, 'recall': MicroRecall: 87.15%}, {'index': 1, 'max': 0.25026464462280273, 'min': 0.0007302761077880859, 'avg': 0.008210160303115844, 'median': 0.006949424743652344, 'length': 10000, 'auc': ROCAUC: 50.00%, 'f1': F1: 94.39%, 'recall': MicroRecall: 89.38%}, {'index': 2, 'max': 0.18382668495178223, 'min': 0.0006763935089111328, 'avg': 0.008446274852752686, 'median': 0.007005214691162109, 'length': 10000, 'auc': ROCAUC: 50.00%, 'f1': F1: 92.34%, 'recall': MicroRecall: 85.76%}, {'index': 3, 'max': 0.15060901641845703, 'min': 0.0009417533874511719, 'avg': 0.006948902559280395, 'median': 0.005710244178771973, 'length': 10000, 'auc': ROCAUC: 50.00%, 'f1': F1: 93.50%, 'recall': MicroRecall: 87.80%}]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "b = os.environ['kafka_bootstrap_servers']\n",
    "u = os.environ['kafka_username']\n",
    "p = os.environ['kafka_password']\n",
    "futures = [f.remote(i,b,u,p) for i in range(4)]\n",
    "out = ray.get(futures)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c32d7f-b3c4-4c2f-866a-4c52c7e71ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e52712-96d4-4a04-a122-3cf410a83160",
   "metadata": {},
   "source": [
    "Every 1000 learning iterations publish the change.\n",
    "\n",
    "1. Version 0 of the model\n",
    "2. Feature Topic\n",
    "3. Prediction Topic\n",
    "4. Learning Topic\n",
    "5. Model Update Topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
